{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d69fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f14bd662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duhu/anaconda3/envs/lora-1/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig, LlamaTokenizer, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d083a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"yahma/llama-13b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c7606b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"这是个测试的\\n\\n\"\n",
    "full_tokens = tokenizer(\n",
    "    text,\n",
    "    truncation=True,\n",
    "    padding=False,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ") # need eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ab71e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 29871, 30810, 30392, 30502, 31851, 31787, 30210,    13,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1336e66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5fd6399",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7621ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f84cf45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29937"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05e0b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'input_ids': [1, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 17506, 366, 975, 29888, 5367, 29973, 13, 13, 2277, 29937, 13291, 29901, 13, 2776, 3236, 12902, 565, 366, 508, 1074, 916, 1234, 29889, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 17506, 366, 975, 29888, 5367, 29973, 13, 13, 2277, 29937, 13291, 29901, 13, 2776, 3236, 12902, 565, 366, 508, 1074, 916, 1234, 29889, 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dfad951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "▁Below\n",
      "▁is\n",
      "▁an\n",
      "▁instruction\n",
      "▁that\n",
      "▁describes\n",
      "▁a\n",
      "▁task\n",
      ".\n",
      "▁Write\n",
      "▁a\n",
      "▁response\n",
      "▁that\n",
      "▁appropri\n",
      "ately\n",
      "▁comple\n",
      "tes\n",
      "▁the\n",
      "▁request\n",
      ".\n",
      "<0x0A>\n",
      "<0x0A>\n",
      "##\n",
      "#\n",
      "▁Inst\n",
      "ruction\n",
      ":\n",
      "<0x0A>\n",
      "Are\n",
      "▁you\n",
      "▁over\n",
      "f\n",
      "itting\n",
      "?\n",
      "<0x0A>\n",
      "<0x0A>\n",
      "##\n",
      "#\n",
      "▁Response\n",
      ":\n",
      "<0x0A>\n",
      "Of\n",
      "▁course\n",
      "▁nah\n",
      "▁if\n",
      "▁you\n",
      "▁can\n",
      "▁see\n",
      "▁other\n",
      "▁answer\n",
      ".\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "raw_data = {\n",
    "    \"instruction\": \"Are you overfitting?\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Of course nah if you can see other answer.\"\n",
    "    }\n",
    "for ele in data['input_ids']:\n",
    "    token = tokenizer.convert_ids_to_tokens(ele)\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd83aa6",
   "metadata": {},
   "source": [
    "**加载训练完的PEFT模型**\n",
    "\n",
    "训练好了一个PEFT模型之后，会在模型输出路径下有一个文件，`adapter_config.json`，这个文件中会记录这个Peft模型的类别等信息，比如lora的配置文件是长这样的：\n",
    "```json\n",
    "{\n",
    "  \"base_model_name_or_path\": \"yahma/llama-7b-hf\",\n",
    "  \"bias\": \"none\",\n",
    "  \"fan_in_fan_out\": false,\n",
    "  \"inference_mode\": true,\n",
    "  \"init_lora_weights\": true,\n",
    "  \"lora_alpha\": 16,\n",
    "  \"lora_dropout\": 0.05,\n",
    "  \"modules_to_save\": null,\n",
    "  \"peft_type\": \"LORA\",\n",
    "  \"r\": 16,\n",
    "  \"target_modules\": [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\"\n",
    "  ],\n",
    "  \"task_type\": \"CAUSAL_LM\"\n",
    "}\n",
    "```\n",
    "所以当我们需要用到这个模型的时候，指定base model以及模型的输出路径，peft库会自动的帮我们去做模型的加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "359b3f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7bb14c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:33<00:00, 11.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-39): 40 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
       "                (lora_A): Linear(in_features=5120, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=5120, bias=False)\n",
       "              )\n",
       "              (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
       "                (lora_A): Linear(in_features=5120, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=5120, bias=False)\n",
       "              )\n",
       "              (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "out_dir = \"models/llama-13b-lora-alpaca-round-0/checkpoint-5900\"\n",
    "base_model = \"yahma/llama-13b-hf\"\n",
    "model = None\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model, \n",
    "    load_in_8bit=False, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model=model, \n",
    "    model_id=out_dir, \n",
    "    torch_dtype=torch.float16)\n",
    "model.half()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "074e20ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/duhu/.cache/huggingface/datasets/BelleGroup___json/BelleGroup--train_3.5M_CN-d0ea45919c9eb506/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversations', 'id'],\n",
      "        num_rows: 3606402\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "data_belle = load_dataset(\"BelleGroup/train_3.5M_CN\")\n",
    "print(data_belle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "623e5660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': '给定一段文本和关键词列表，删除文本中包含所有给定关键词的子字符串。\\n文本：\"这是一个测试句子，目的是看看模型是否可以正确地从这个句子中删除关键词。\"\\\\n关键词列表：[‘测试’，‘模型’]'},\n",
       "  {'from': 'assistant',\n",
       "   'value': '删除包含所有给定关键词的子字符串后，文本变为：\"这是一个句子，目的是看看是否可以正确地从这个句子中删除关键词。\"'},\n",
       "  {'from': 'human', 'value': '好的。现在请你将这个文本中的所有的逗号都替换成空格。'},\n",
       "  {'from': 'assistant',\n",
       "   'value': '好的，请稍等一下，现在我会将文本中的所有逗号替换为空格。处理后文本为：\"这是一个句子 目的是看看是否可以正确地从这个句子中删除关键词。\"。处理结果如何？'}],\n",
       " 'id': '16012449'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_belle['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e851fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The following is a conversation between an AI assistant called Doer and a human user called User. The assistant is intelligent, knowledgeable and polite to answer questions of user.\\n\\nUser: 你是谁？\\n\\nDoer: '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "prompt_pre = (\n",
    "    \"The following is a conversation between an AI assistant called Doer and a human user called User. \"\n",
    "    \"The assistant is intelligent, knowledgeable and polite to answer questions of user.\\n\\n\"\n",
    ")\n",
    "prompt_history = \"User: {input}\\n\\nDoer: {output}\\n\\n\"\n",
    "prompt_post = \"User: {input}\\n\\nDoer: \"\n",
    "\n",
    "def generate_prompt(data_point, stage='val'):\n",
    "    user_prompt = prompt_pre # 固定开场白\n",
    "    # 这里面的字段是conversions，而不是input，因为上面的例子的字段是conversations\n",
    "    conversations = data_point['conversations']\n",
    "    # 获取多轮对话的轮数\n",
    "#     assert len(conversations) % 2 == 0, f\"{data_point} not compeleted finised the conversation\"\n",
    "    num_turns = max(len(conversations) // 2, 1)\n",
    "    print(num_turns)\n",
    "#     for i in range(num_turns - 1): # 最后一轮对话单独处理，此处不处理\n",
    "#         assert conversations[2*i]['from'] == \"human\"\n",
    "#         assert conversations[2*i+1]['from'] == \"assistant\"\n",
    "#         human = conversations[2*i]['value']\n",
    "#         assistant = conversations[2*i+1]['value']\n",
    "#         user_prompt += prompt_history.format_map({'input': human, 'output': assistant})\n",
    "    # 添加最后一轮对话的输入部分\n",
    "    user_prompt += prompt_post.format_map({'input': conversations[2*num_turns-2]['value']})\n",
    "    # 根据是训练还是推理，用不同的方式来处理最后一轮对话的回答部分\n",
    "    if stage == 'train':\n",
    "        user_prompt += conversations[2*num_turns-1]['value']\n",
    "    \n",
    "    return {\"prompt\": user_prompt}\n",
    "\n",
    "data = {\n",
    "  \"conversations\": [\n",
    "    {\n",
    "      \"from\": \"human\",\n",
    "      \"value\": \"你是谁？\"\n",
    "    }\n",
    "  ],\n",
    "  \"id\": \"16012449\"\n",
    "}\n",
    "one_prompter_raw = generate_prompt(data)\n",
    "one_prompter = one_prompter_raw['prompt']\n",
    "one_prompter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6497ea64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   450,  1494,   338,   263, 14983,  1546,   385,   319, 29902,\n",
      "         20255,  2000,  1938,   261,   322,   263,  5199,  1404,  2000,  4911,\n",
      "         29889,   450, 20255,   338, 13052,   296, 29892,  7134,   519,   322,\n",
      "          1248,   568,   304,  1234,  5155,   310,  1404, 29889,    13,    13,\n",
      "          2659, 29901, 29871, 30919, 30392,   235,   179,   132, 30882,    13,\n",
      "            13,  6132,   261, 29901, 29871]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# some parameters\n",
    "temperature=.1\n",
    "top_p=0.75\n",
    "top_k=40\n",
    "num_beams=4\n",
    "max_new_tokens=256\n",
    "\n",
    "# generate config\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    "    num_beams=num_beams,\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    repetition_penalty=2.,\n",
    "    bad_words_ids=tokenizer(['\\n\\nUser:','\\n\\Doer:'], add_special_tokens=False).input_ids,\n",
    ")\n",
    "\n",
    "# get inputs\n",
    "inputs = tokenizer(one_prompter, return_tensors='pt')\n",
    "print(inputs)\n",
    "input_ids = inputs['input_ids'].to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids = input_ids,\n",
    "        generation_config = generation_config,\n",
    "        return_dict_in_generate = True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "\n",
    "s = generation_output.sequences[0]\n",
    "output = tokenizer.decode(s)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d7d10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "import copy\n",
    "\n",
    "class prompt:\n",
    "    def __init__(self, tokenizer, max_len, add_eos=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.add_eos=add_eos\n",
    "\n",
    "\n",
    "class chat_prompt(prompt):\n",
    "    prompt_pre = (\n",
    "        \"The following is a conversation between an AI assistant called Assistant and a human user called User. \"\n",
    "        \"The assistant is intelligent, knowledgeable and polite to answer questions of user.\\n\\n\"\n",
    "    )\n",
    "    prompt_history = \"User:{input}\\n\\nAssistant:{output}\\n\\n\"\n",
    "    prompt_post = \"User:{input}\\n\\nAssistant:\"\n",
    "\n",
    "    def preprocess_gen(self, data_point):\n",
    "        user_prompt = self.prompt_pre\n",
    "        len_avail = self.max_len - len(self.tokenizer(user_prompt, add_special_tokens=False)['input_ids'])\n",
    "        input_prompt = self.prompt_post.format_map({'input':data_point['input']})\n",
    "        len_avail -= len(self.tokenizer(input_prompt, add_special_tokens=False)['input_ids'])\n",
    "        lens = len(data_point['history'])\n",
    "        tokenized_lens = []\n",
    "        for i in range(lens):\n",
    "            tmp_prompt = self.prompt_history.format_map(data_point['history'][i])\n",
    "            tokenized_lens.append(len(self.tokenizer(tmp_prompt,add_special_tokens=False)[\"input_ids\"]))\n",
    "        \n",
    "        # 启发式：/2 优先除前面的\n",
    "        i = 0\n",
    "        while sum(tokenized_lens) > len_avail and i < lens:\n",
    "            history = data_point['history'][i]\n",
    "            tmp_len1 = len(history['input'])\n",
    "            tmp_len2 = len(history['output'])\n",
    "            if tmp_len2 > tmp_len1:\n",
    "                history['output'] = history['output'][:tmp_len2//2]\n",
    "            else:\n",
    "                history['input'] = history['input'][:tmp_len1//2]\n",
    "            prompt = self.prompt_history.format_map(history)\n",
    "            single_len =(len(self.tokenizer(prompt,add_special_tokens=False)[\"input_ids\"]))\n",
    "            tokenized_lens[i] = single_len\n",
    "            i += 1\n",
    "        total_len = sum(tokenized_lens)\n",
    "        # 还不够的话 直接截断\n",
    "        while total_len > len_avail and i < lens - 1 :\n",
    "            total_len -= tokenized_lens[i]\n",
    "            data_point['history'] = data_point['history'][1:]\n",
    "            i += 1\n",
    "        # 最终合并\n",
    "        for i in range(lens):\n",
    "            user_prompt += self.prompt_history.format_map(data_point['history'][i])\n",
    "        user_prompt += input_prompt\n",
    "        print({'real_input:':user_prompt})\n",
    "        inputs = self.tokenizer(user_prompt)[\"input_ids\"]\n",
    "        return inputs\n",
    "\n",
    "    def preprocess_train(self, data_point):\n",
    "        user_prompt = self.prompt_pre\n",
    "        lens = len(data_point['input'])\n",
    "        for i in range(lens-1):\n",
    "            user_prompt += self.prompt_history.format_map({'input':data_point['input'][i].strip(),'output':data_point['output'][i].strip()})\n",
    "        user_prompt += self.prompt_post.format_map({'input':data_point['input'][-1].strip()})\n",
    "\n",
    "        len_user_prompt_tokens = len(self.tokenizer(\n",
    "            user_prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "        )[\"input_ids\"]) - 1 # remove extra eos\n",
    "        if self.add_eos:\n",
    "            full_tokens = self.tokenizer(\n",
    "                user_prompt + data_point[\"output\"][-1].strip(),\n",
    "                truncation=True,\n",
    "                padding=False,\n",
    "                max_length=self.max_len,\n",
    "            )[\"input_ids\"] # need eos\n",
    "        else:\n",
    "            full_tokens = self.tokenizer(\n",
    "                user_prompt + data_point[\"output\"][-1].strip(),\n",
    "                truncation=True,\n",
    "                padding=False,\n",
    "                max_length=self.max_len+1,\n",
    "            )[\"input_ids\"][:-1] # delete eos\n",
    "        return {\n",
    "            \"input_ids\": full_tokens,\n",
    "            \"labels\": [-100] * len_user_prompt_tokens + full_tokens[len_user_prompt_tokens:],\n",
    "            \"attention_mask\": [1] * (len(full_tokens)),\n",
    "        }\n",
    "\n",
    "    def data_collator(self,):\n",
    "        return transformers.DataCollatorForSeq2Seq(self.tokenizer)\n",
    "\n",
    "    def postprocess(self, text, render=False):\n",
    "        output = text.split(\"Assistant:\")[-1].strip()\n",
    "        if 'User:' in output:\n",
    "            output = output.split(\"User:\")[0]\n",
    "        output = output.replace('�','') \n",
    "        if render:\n",
    "            # fix gradio chatbot markdown code render bug\n",
    "            lines = output.split(\"\\n\")\n",
    "            for i, line in enumerate(lines):\n",
    "                if \"```\" in line:\n",
    "                    if line != \"```\":\n",
    "                        lines[i] = f'<pre><code class=\"language-{lines[i][3:]}\">'\n",
    "                    else:\n",
    "                        lines[i] = '</code></pre>'\n",
    "                else:\n",
    "                    if i > 0:\n",
    "                        lines[i] = \"<br/>\" + line.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").replace(\"__\", '\\_\\_')\n",
    "            output =  \"\".join(lines)\n",
    "            # output = output.replace('<br/><pre>','\\n<pre>') work for html; but not for gradio\n",
    "        return output\n",
    "\n",
    "    def get_data_collator():\n",
    "        return transformers.DataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "589ea71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_point = {\n",
    "    \"input\": [\"A B C D E F G H\", \"J K L M N O P Q\"],\n",
    "    \"output\": [\" 1 2 3\", \"4 5 6\"]\n",
    "}\n",
    "prompt = chat_prompt(tokenizer, max_len=128, add_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9260424",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 450, 1494, 338, 263, 14983, 1546, 385, 319, 29902, 20255, 2000, 4007, 22137, 322, 263, 5199, 1404, 2000, 4911, 29889, 450, 20255, 338, 13052, 296, 29892, 7134, 519, 322, 1248, 568, 304, 1234, 5155, 310, 1404, 29889, 13, 13, 2659, 29901, 29909, 350, 315, 360, 382, 383, 402, 379, 13, 13, 7900, 22137, 29901, 29896, 29871, 29906, 29871, 29941, 13, 13, 2659, 29901, 29967, 476, 365, 341, 405, 438, 349, 660, 13, 13, 7900, 22137, 29901, 29946, 29871, 29945, 29871, 29953], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 29901, 29946, 29871, 29945, 29871, 29953], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(prompt.preprocess_train(data_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "497d71ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(29953)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb91b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
